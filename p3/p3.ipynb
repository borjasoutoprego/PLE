{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, LSTM, TimeDistributed, Bidirectional\n",
    "from nervaluate import Evaluator\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class txtReader:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def read_split(self):\n",
    "        with open(self.filename, 'r') as f:\n",
    "            file_read = f.read()\n",
    "        lines = file_read.split('\\n')\n",
    "        text = []\n",
    "        text_id = []\n",
    "        tmp = []\n",
    "        tmp_id = []\n",
    "        for i in lines:\n",
    "            if i == '':\n",
    "                if len(tmp) != 0:\n",
    "                    text.append(tmp)\n",
    "                    text_id.append(tmp_id)\n",
    "                    tmp = []\n",
    "                    tmp_id = []\n",
    "            else:\n",
    "                word = i.split('\\t')\n",
    "                tmp.append(word[0])\n",
    "                tmp_id.append(word[1])\n",
    "\n",
    "        return text, text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alphabet:\n",
    "    def __init__(self, train_file, dev_file, test_file):\n",
    "        self.train_file = train_file\n",
    "        self.dev_file = dev_file\n",
    "        self.test_file = test_file\n",
    "        self.data = dict()\n",
    "        self.labels = dict()\n",
    "\n",
    "    def read_split(self):\n",
    "        text_files = []\n",
    "        for text_file in [self.train_file, self.dev_file, self.test_file]:\n",
    "            txt = txtReader(text_file)\n",
    "            text, text_id = txt.read_split()\n",
    "            text_files.append(text)\n",
    "            text_files.append(text_id)\n",
    "\n",
    "        return text_files[0], text_files[1], text_files[2], text_files[3], text_files[4], text_files[5]\n",
    "    \n",
    "    def _tagger(self, dataset, cnt, dictionary):\n",
    "        for i in dataset: # i es una frase\n",
    "            for j in i: # j es una palabra\n",
    "                pos = i.index(j) # pos es la posicion de la palabra en la frase\n",
    "                if j not in dictionary:\n",
    "                    dictionary[j] = cnt\n",
    "                    i[pos] = cnt\n",
    "                    cnt += 1\n",
    "                else:\n",
    "                    i[pos] = dictionary[j]\n",
    "\n",
    "        return dataset, cnt, dictionary \n",
    "\n",
    "    def labelEncoder(self):\n",
    "        train, train_id, dev, dev_id, test, test_id = self.read_split()\n",
    "        cnt = 1\n",
    "        cnt_id = 0\n",
    "\n",
    "        train, cnt, self.data = self._tagger(train, cnt, self.data) \n",
    "        train_id, cnt_id, self.labels = self._tagger(train_id, cnt_id, self.labels)\n",
    "        dev, cnt, self.data = self._tagger(dev, cnt, self.data)\n",
    "        dev_id, cnt_id, self.labels = self._tagger(dev_id, cnt_id, self.labels)\n",
    "        \n",
    "        len_train = 0\n",
    "        for i in train:\n",
    "            len_train += len(i)\n",
    "\n",
    "        for phrase_te in test:\n",
    "            for te in phrase_te:\n",
    "                pos = phrase_te.index(te)\n",
    "                if te not in self.data:\n",
    "                    self.data[te] = len_train\n",
    "                    phrase_te[pos] = self.data[te]\n",
    "                else:\n",
    "                    phrase_te[pos] = self.data[te]\n",
    "\n",
    "        for phrase_te_id in test_id:\n",
    "            for te_id in phrase_te_id:\n",
    "                pos_id = phrase_te_id.index(te_id)\n",
    "                if te_id not in self.labels:\n",
    "                    self.labels[te_id] = len_train\n",
    "                    phrase_te_id[pos_id] = self.labels[te_id]\n",
    "                else:\n",
    "                    phrase_te_id[pos_id] = self.labels[te_id] \n",
    "\n",
    "        return train, train_id, dev, dev_id, test, test_id, self.data, self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, test_PartTUT, test_id_PartTUT, PartTUT_data, PartTUT_labels = alphabet('materiales_practica/datasets/PartTUT/train.txt', 'materiales_practica/datasets/PartTUT/dev.txt', 'materiales_practica/datasets/PartTUT/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MITMovie, train_id_MITMovie, dev_MITMovie, dev_id_MITMovie, test_MITMovie, test_id_MITMovie, MITMovie_data, MITMovie_labels = alphabet('materiales_practica/datasets/MITMovie/train.txt', 'materiales_practica/datasets/MITMovie/dev.txt', 'materiales_practica/datasets/MITMovie/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MITRestaurant, train_id_MITRestaurant, dev_MITRestaurant, dev_id_MITRestaurant, test_MITRestaurant, test_id_MITRestaurant, MITRestaurant_data, MITRestaurant_labels = alphabet('materiales_practica/datasets/MITRestaurant/train.txt', 'materiales_practica/datasets/MITRestaurant/dev.txt', 'materiales_practica/datasets/MITRestaurant/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTagger():\n",
    "    def __init__(self, train, train_id, dev, dev_id, test, test_id, labels_dict, n, loss, optimizer, metrics): # weighted_metrics\n",
    "        self.model = Sequential()\n",
    "        self.train = train\n",
    "        self.train_id = train_id\n",
    "        self.dev = dev\n",
    "        self.dev_id = dev_id\n",
    "        self.test = test\n",
    "        self.test_id = test_id\n",
    "        self.labels_dict = labels_dict\n",
    "        self.n = n\n",
    "        len_train = 0\n",
    "        for i in train:\n",
    "            len_train += len(i)\n",
    "        self.vocab_size = len_train + 1 # +1 por valores desconocidos en test\n",
    "        self.classes = labels_dict.keys()\n",
    "        self.num_classes = len(labels_dict)\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        # self.weighted_metrics = weighted_metrics\n",
    "        self.train_windows = []\n",
    "        self.dev_windows = []\n",
    "        self.test_windows = []\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def build_model(self): \n",
    "        self.model.add(Input(shape=(self.n*2+1,), dtype=tf.int32))\n",
    "        self.model.add(Embedding(input_dim = self.vocab_size, output_dim=20, mask_zero=True, input_length=self.n*2+1))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "    def train_model(self):\n",
    "        # Añadimos padding a las frases y las dividimos en ventanas de tamaño 2n+1\n",
    "        padding = []\n",
    "        for i in range(self.n):\n",
    "            padding.append(0)\n",
    "\n",
    "        padded_train = []\n",
    "        padded_dev = []\n",
    "        one_hot_train_id = []\n",
    "        one_hot_dev_id = []\n",
    "        for j in range(len(self.train)):\n",
    "            padded_phrase = padding + self.train[j] + padding\n",
    "            padded_train.append(padded_phrase) # padding\n",
    "            for w in range(self.n, len(padded_phrase)-self.n): # división en ventanas\n",
    "                self.train_windows.append(padded_phrase[w-self.n:w+self.n+1])\n",
    "            one_hot_phrase = to_categorical(self.train_id[j], num_classes=self.num_classes) # one-hot encoding\n",
    "            for v in one_hot_phrase:\n",
    "                one_hot_train_id.append(v)\n",
    "\n",
    "        for k in range(len(self.dev)):\n",
    "            padded_phrase_dev = padding + self.dev[k] + padding\n",
    "            padded_dev.append(padded_phrase_dev)\n",
    "            for w in range(self.n, len(padded_phrase_dev)-self.n):\n",
    "                self.dev_windows.append(padded_phrase_dev[w-self.n:w+self.n+1])\n",
    "            one_hot_phrase_dev = to_categorical(self.dev_id[k], num_classes=self.num_classes)\n",
    "            for v in one_hot_phrase_dev:\n",
    "                one_hot_dev_id.append(v)\n",
    "        \n",
    "        # Convertimos las listas en tensores\n",
    "        train_tensor = tf.data.Dataset.from_tensor_slices((self.train_windows, one_hot_train_id))\n",
    "        train_tensor = train_tensor.batch(self.batch_size)\n",
    "        dev_tensor = tf.data.Dataset.from_tensor_slices((self.dev_windows, one_hot_dev_id))\n",
    "        dev_tensor = dev_tensor.batch(self.batch_size)\n",
    "\n",
    "        # Reescribimos las variables globales\n",
    "        self.train = padded_train\n",
    "        self.dev = padded_dev\n",
    "\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "        self.model.fit(train_tensor, epochs=1, validation_data=dev_tensor, verbose=1)\n",
    "\n",
    "    def evaluate_model(self, task):\n",
    "        padding = []\n",
    "        for i in range(self.n):\n",
    "            padding.append(0)\n",
    "        \n",
    "        # Añadimos el padding, dividimos en ventanas y hacemos one-hot encoding de las etiquetas\n",
    "        padded_test = []\n",
    "        one_hot_test_id = []\n",
    "        for j in range(len(self.test)):\n",
    "            padded_phrase_test = padding + self.test[j] + padding\n",
    "            padded_test.append(padded_phrase_test)\n",
    "            for w in range(self.n, len(padded_phrase_test)-self.n): # división en ventanas\n",
    "                self.test_windows.append(padded_phrase_test[w-self.n:w+self.n+1])\n",
    "            one_hot_phrase_test = to_categorical(self.test_id[j], num_classes=self.num_classes) # one-hot encoding\n",
    "            for v in one_hot_phrase_test:\n",
    "                one_hot_test_id.append(v)\n",
    "\n",
    "        test_labels = [] # guarda las etiquetas como cadena de caracteres\n",
    "        test_labels_length = [] # guarda la longitud de las oraciones\n",
    "        for i in self.test_id: \n",
    "            temp_phrase = []\n",
    "            for j in i: \n",
    "                for k, v in self.labels_dict.items():\n",
    "                    if j == v:\n",
    "                        temp_phrase.append(k)\n",
    "            test_labels.append(temp_phrase)\n",
    "            test_labels_length.append(len(temp_phrase))\n",
    "        \n",
    "        one_hot_dict = dict() # Almacena la correspondencia entre one hot (valor) y etiqueta numérica (clave)\n",
    "        pos_one_hot = 0\n",
    "        for i in self.test_id:\n",
    "            for j in range(len(i)):\n",
    "                if i[j] not in one_hot_dict:\n",
    "                    one_hot_dict[i[j]] = np.argmax(one_hot_test_id[pos_one_hot]) # devuelve el índice del elemento con valor 1 en one_hot_test_id[i]\n",
    "                pos_one_hot += 1\n",
    "\n",
    "        test_tensor = tf.data.Dataset.from_tensor_slices((self.test_windows, one_hot_test_id))\n",
    "        test_tensor = test_tensor.batch(self.batch_size)\n",
    "        \n",
    "        if task == \"PoS\":\n",
    "            loss, accuracy = self.model.evaluate(test_tensor, verbose=1)\n",
    "            return loss, accuracy\n",
    "        elif task == \"NER\":\n",
    "            loss, accuracy = self.model.evaluate(test_tensor, verbose=1)\n",
    "            pred = self.model.predict(test_tensor).astype(np.float32)\n",
    "\n",
    "            # para cada elemento de pred obtenemos la etiqueta numérica  \n",
    "            predictions = []\n",
    "            for p in pred:\n",
    "                pos = np.argmax(p)\n",
    "                for k, v in one_hot_dict.items():\n",
    "                    if pos == v:\n",
    "                        predictions.append(k)\n",
    "            \n",
    "            # convertimos las etiquetas numéricas a etiquetas de texto\n",
    "            pred_labels = []\n",
    "            test_index = 0\n",
    "            tmp_phrase = []\n",
    "            for l in predictions:\n",
    "                for k, v in self.labels_dict.items():\n",
    "                    if l == v:\n",
    "                        tmp_phrase.append(k)\n",
    "                    if len(tmp_phrase) == test_labels_length[test_index]:\n",
    "                        pred_labels.append(tmp_phrase)\n",
    "                        tmp_phrase = []\n",
    "                        test_index += 1\n",
    "                        break\n",
    "\n",
    "            numeric_tags = set()\n",
    "            for i in self.train_id:\n",
    "                numeric_tags = set(numeric_tags|set(i))\n",
    "            for j in self.dev_id:\n",
    "                numeric_tags = set(numeric_tags|set(j))\n",
    "            numeric_tags = list(numeric_tags)\n",
    "\n",
    "            # convertimos las etiquetas numéricas a etiquetas de texto\n",
    "            tags = []\n",
    "            for t in numeric_tags:\n",
    "                for k, v in self.labels_dict.items():\n",
    "                    if t == v:\n",
    "                        tags.append(k)\n",
    "\n",
    "            # Nos quedamos con las etiquetas de la entidad (sin B e I)\n",
    "            evaluator_tags = []\n",
    "            for tag in tags:\n",
    "                if tag == \"O\":\n",
    "                    continue\n",
    "                elif tag == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    evaluator_tag = tag[2:]\n",
    "\n",
    "                if evaluator_tag not in evaluator_tags:\n",
    "                    evaluator_tags.append(evaluator_tag)\n",
    "            \n",
    "            evaluator = Evaluator(test_labels, pred=pred_labels, tags=evaluator_tags, loader=\"list\")\n",
    "            results, results_by_tag = evaluator.evaluate()\n",
    "            \n",
    "            return loss, accuracy, results, results_by_tag\n",
    "        else:\n",
    "            return \"Task not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43494 vocab_size\n",
      "680/680 [==============================] - 10s 14ms/step - loss: 1.1954 - accuracy: 0.6697 - val_loss: 0.6266 - val_accuracy: 0.8002\n"
     ]
    }
   ],
   "source": [
    "modelPartTUT = FFTagger(train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, test_PartTUT, test_id_PartTUT,  PartTUT_labels, 2, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "modelPartTUT.build_model()\n",
    "modelPartTUT.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 1ms/step - loss: 0.5380 - accuracy: 0.8398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5380165576934814, 0.8398101329803467)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelPartTUT.evaluate_model(\"PoS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14283/14283 [==============================] - 914s 64ms/step - loss: 0.7141 - accuracy: 0.7890 - val_loss: 0.5518 - val_accuracy: 0.8295\n"
     ]
    }
   ],
   "source": [
    "modelMITMovie = FFTagger(train_MITMovie, train_id_MITMovie, dev_MITMovie, dev_id_MITMovie, test_MITMovie, test_id_MITMovie, MITMovie_labels, 2, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "modelMITMovie.build_model()\n",
    "modelMITMovie.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelMITMovie.evaluate_model(\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 35s 35ms/step - loss: 0.9973 - accuracy: 0.7379 - val_loss: 0.5626 - val_accuracy: 0.8487\n"
     ]
    }
   ],
   "source": [
    "modelMITRestaurant = FFTagger(train_MITRestaurant, train_id_MITRestaurant, dev_MITRestaurant, dev_id_MITRestaurant, test_MITRestaurant, test_id_MITRestaurant, MITRestaurant_labels, 2, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "modelMITRestaurant.build_model()\n",
    "modelMITRestaurant.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/223 [==============================] - 1s 2ms/step - loss: 0.5831 - accuracy: 0.8437\n",
      "223/223 [==============================] - 1s 2ms/step\n",
      "1521 test_labels 1521 pred_labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5831315517425537,\n",
       " 0.8437149524688721,\n",
       " {'ent_type': {'correct': 2411,\n",
       "   'incorrect': 569,\n",
       "   'partial': 0,\n",
       "   'missed': 171,\n",
       "   'spurious': 605,\n",
       "   'possible': 3151,\n",
       "   'actual': 3585,\n",
       "   'precision': 0.6725244072524407,\n",
       "   'recall': 0.7651539193906697,\n",
       "   'f1': 0.7158551068883611},\n",
       "  'partial': {'correct': 2238,\n",
       "   'incorrect': 0,\n",
       "   'partial': 742,\n",
       "   'missed': 171,\n",
       "   'spurious': 605,\n",
       "   'possible': 3151,\n",
       "   'actual': 3585,\n",
       "   'precision': 0.7277545327754533,\n",
       "   'recall': 0.827991113932085,\n",
       "   'f1': 0.774643705463183},\n",
       "  'strict': {'correct': 1950,\n",
       "   'incorrect': 1030,\n",
       "   'partial': 0,\n",
       "   'missed': 171,\n",
       "   'spurious': 605,\n",
       "   'possible': 3151,\n",
       "   'actual': 3585,\n",
       "   'precision': 0.5439330543933054,\n",
       "   'recall': 0.6188511583624247,\n",
       "   'f1': 0.578978622327791},\n",
       "  'exact': {'correct': 2238,\n",
       "   'incorrect': 742,\n",
       "   'partial': 0,\n",
       "   'missed': 171,\n",
       "   'spurious': 605,\n",
       "   'possible': 3151,\n",
       "   'actual': 3585,\n",
       "   'precision': 0.6242677824267783,\n",
       "   'recall': 0.7102507140590288,\n",
       "   'f1': 0.6644893111638954}},\n",
       " {'Rating': {'ent_type': {'correct': 167,\n",
       "    'incorrect': 27,\n",
       "    'partial': 0,\n",
       "    'missed': 7,\n",
       "    'spurious': 35,\n",
       "    'possible': 201,\n",
       "    'actual': 229,\n",
       "    'precision': 0.7292576419213974,\n",
       "    'recall': 0.8308457711442786,\n",
       "    'f1': 0.7767441860465116},\n",
       "   'partial': {'correct': 140,\n",
       "    'incorrect': 0,\n",
       "    'partial': 54,\n",
       "    'missed': 7,\n",
       "    'spurious': 35,\n",
       "    'possible': 201,\n",
       "    'actual': 229,\n",
       "    'precision': 0.7292576419213974,\n",
       "    'recall': 0.8308457711442786,\n",
       "    'f1': 0.7767441860465116},\n",
       "   'strict': {'correct': 127,\n",
       "    'incorrect': 67,\n",
       "    'partial': 0,\n",
       "    'missed': 7,\n",
       "    'spurious': 35,\n",
       "    'possible': 201,\n",
       "    'actual': 229,\n",
       "    'precision': 0.5545851528384279,\n",
       "    'recall': 0.6318407960199005,\n",
       "    'f1': 0.5906976744186047},\n",
       "   'exact': {'correct': 140,\n",
       "    'incorrect': 54,\n",
       "    'partial': 0,\n",
       "    'missed': 7,\n",
       "    'spurious': 35,\n",
       "    'possible': 201,\n",
       "    'actual': 229,\n",
       "    'precision': 0.611353711790393,\n",
       "    'recall': 0.6965174129353234,\n",
       "    'f1': 0.6511627906976744}},\n",
       "  'Location': {'ent_type': {'correct': 758,\n",
       "    'incorrect': 30,\n",
       "    'partial': 0,\n",
       "    'missed': 24,\n",
       "    'spurious': 91,\n",
       "    'possible': 812,\n",
       "    'actual': 879,\n",
       "    'precision': 0.8623435722411832,\n",
       "    'recall': 0.9334975369458128,\n",
       "    'f1': 0.8965109402720283},\n",
       "   'partial': {'correct': 624,\n",
       "    'incorrect': 0,\n",
       "    'partial': 164,\n",
       "    'missed': 24,\n",
       "    'spurious': 91,\n",
       "    'possible': 812,\n",
       "    'actual': 879,\n",
       "    'precision': 0.8031854379977247,\n",
       "    'recall': 0.8694581280788177,\n",
       "    'f1': 0.8350088704908338},\n",
       "   'strict': {'correct': 611,\n",
       "    'incorrect': 177,\n",
       "    'partial': 0,\n",
       "    'missed': 24,\n",
       "    'spurious': 91,\n",
       "    'possible': 812,\n",
       "    'actual': 879,\n",
       "    'precision': 0.6951080773606371,\n",
       "    'recall': 0.7524630541871922,\n",
       "    'f1': 0.722649319929036},\n",
       "   'exact': {'correct': 624,\n",
       "    'incorrect': 164,\n",
       "    'partial': 0,\n",
       "    'missed': 24,\n",
       "    'spurious': 91,\n",
       "    'possible': 812,\n",
       "    'actual': 879,\n",
       "    'precision': 0.7098976109215017,\n",
       "    'recall': 0.7684729064039408,\n",
       "    'f1': 0.7380248373743348}},\n",
       "  'Restaurant_Name': {'ent_type': {'correct': 195,\n",
       "    'incorrect': 181,\n",
       "    'partial': 0,\n",
       "    'missed': 26,\n",
       "    'spurious': 159,\n",
       "    'possible': 402,\n",
       "    'actual': 535,\n",
       "    'precision': 0.3644859813084112,\n",
       "    'recall': 0.48507462686567165,\n",
       "    'f1': 0.41622198505869795},\n",
       "   'partial': {'correct': 209,\n",
       "    'incorrect': 0,\n",
       "    'partial': 167,\n",
       "    'missed': 26,\n",
       "    'spurious': 159,\n",
       "    'possible': 402,\n",
       "    'actual': 535,\n",
       "    'precision': 0.5467289719626168,\n",
       "    'recall': 0.7276119402985075,\n",
       "    'f1': 0.624332977588047},\n",
       "   'strict': {'correct': 147,\n",
       "    'incorrect': 229,\n",
       "    'partial': 0,\n",
       "    'missed': 26,\n",
       "    'spurious': 159,\n",
       "    'possible': 402,\n",
       "    'actual': 535,\n",
       "    'precision': 0.27476635514018694,\n",
       "    'recall': 0.3656716417910448,\n",
       "    'f1': 0.3137673425827108},\n",
       "   'exact': {'correct': 209,\n",
       "    'incorrect': 167,\n",
       "    'partial': 0,\n",
       "    'missed': 26,\n",
       "    'spurious': 159,\n",
       "    'possible': 402,\n",
       "    'actual': 535,\n",
       "    'precision': 0.39065420560747666,\n",
       "    'recall': 0.5199004975124378,\n",
       "    'f1': 0.44610458911419426}},\n",
       "  'Price': {'ent_type': {'correct': 63,\n",
       "    'incorrect': 95,\n",
       "    'partial': 0,\n",
       "    'missed': 13,\n",
       "    'spurious': 0,\n",
       "    'possible': 171,\n",
       "    'actual': 158,\n",
       "    'precision': 0.3987341772151899,\n",
       "    'recall': 0.3684210526315789,\n",
       "    'f1': 0.3829787234042553},\n",
       "   'partial': {'correct': 120,\n",
       "    'incorrect': 0,\n",
       "    'partial': 38,\n",
       "    'missed': 13,\n",
       "    'spurious': 0,\n",
       "    'possible': 171,\n",
       "    'actual': 158,\n",
       "    'precision': 0.879746835443038,\n",
       "    'recall': 0.8128654970760234,\n",
       "    'f1': 0.844984802431611},\n",
       "   'strict': {'correct': 61,\n",
       "    'incorrect': 97,\n",
       "    'partial': 0,\n",
       "    'missed': 13,\n",
       "    'spurious': 0,\n",
       "    'possible': 171,\n",
       "    'actual': 158,\n",
       "    'precision': 0.3860759493670886,\n",
       "    'recall': 0.3567251461988304,\n",
       "    'f1': 0.3708206686930091},\n",
       "   'exact': {'correct': 120,\n",
       "    'incorrect': 38,\n",
       "    'partial': 0,\n",
       "    'missed': 13,\n",
       "    'spurious': 0,\n",
       "    'possible': 171,\n",
       "    'actual': 158,\n",
       "    'precision': 0.759493670886076,\n",
       "    'recall': 0.7017543859649122,\n",
       "    'f1': 0.729483282674772}},\n",
       "  'Hours': {'ent_type': {'correct': 168,\n",
       "    'incorrect': 24,\n",
       "    'partial': 0,\n",
       "    'missed': 20,\n",
       "    'spurious': 54,\n",
       "    'possible': 212,\n",
       "    'actual': 246,\n",
       "    'precision': 0.6829268292682927,\n",
       "    'recall': 0.7924528301886793,\n",
       "    'f1': 0.7336244541484718},\n",
       "   'partial': {'correct': 136,\n",
       "    'incorrect': 0,\n",
       "    'partial': 56,\n",
       "    'missed': 20,\n",
       "    'spurious': 54,\n",
       "    'possible': 212,\n",
       "    'actual': 246,\n",
       "    'precision': 0.6666666666666666,\n",
       "    'recall': 0.7735849056603774,\n",
       "    'f1': 0.7161572052401747},\n",
       "   'strict': {'correct': 123,\n",
       "    'incorrect': 69,\n",
       "    'partial': 0,\n",
       "    'missed': 20,\n",
       "    'spurious': 54,\n",
       "    'possible': 212,\n",
       "    'actual': 246,\n",
       "    'precision': 0.5,\n",
       "    'recall': 0.5801886792452831,\n",
       "    'f1': 0.537117903930131},\n",
       "   'exact': {'correct': 136,\n",
       "    'incorrect': 56,\n",
       "    'partial': 0,\n",
       "    'missed': 20,\n",
       "    'spurious': 54,\n",
       "    'possible': 212,\n",
       "    'actual': 246,\n",
       "    'precision': 0.5528455284552846,\n",
       "    'recall': 0.6415094339622641,\n",
       "    'f1': 0.5938864628820961}},\n",
       "  'Dish': {'ent_type': {'correct': 208,\n",
       "    'incorrect': 70,\n",
       "    'partial': 0,\n",
       "    'missed': 10,\n",
       "    'spurious': 67,\n",
       "    'possible': 288,\n",
       "    'actual': 345,\n",
       "    'precision': 0.6028985507246377,\n",
       "    'recall': 0.7222222222222222,\n",
       "    'f1': 0.6571879936808847},\n",
       "   'partial': {'correct': 216,\n",
       "    'incorrect': 0,\n",
       "    'partial': 62,\n",
       "    'missed': 10,\n",
       "    'spurious': 67,\n",
       "    'possible': 288,\n",
       "    'actual': 345,\n",
       "    'precision': 0.7159420289855073,\n",
       "    'recall': 0.8576388888888888,\n",
       "    'f1': 0.7804107424960506},\n",
       "   'strict': {'correct': 170,\n",
       "    'incorrect': 108,\n",
       "    'partial': 0,\n",
       "    'missed': 10,\n",
       "    'spurious': 67,\n",
       "    'possible': 288,\n",
       "    'actual': 345,\n",
       "    'precision': 0.4927536231884058,\n",
       "    'recall': 0.5902777777777778,\n",
       "    'f1': 0.5371248025276462},\n",
       "   'exact': {'correct': 216,\n",
       "    'incorrect': 62,\n",
       "    'partial': 0,\n",
       "    'missed': 10,\n",
       "    'spurious': 67,\n",
       "    'possible': 288,\n",
       "    'actual': 345,\n",
       "    'precision': 0.6260869565217392,\n",
       "    'recall': 0.75,\n",
       "    'f1': 0.6824644549763035}},\n",
       "  'Amenity': {'ent_type': {'correct': 418,\n",
       "    'incorrect': 64,\n",
       "    'partial': 0,\n",
       "    'missed': 51,\n",
       "    'spurious': 135,\n",
       "    'possible': 533,\n",
       "    'actual': 617,\n",
       "    'precision': 0.6774716369529984,\n",
       "    'recall': 0.7842401500938087,\n",
       "    'f1': 0.7269565217391305},\n",
       "   'partial': {'correct': 351,\n",
       "    'incorrect': 0,\n",
       "    'partial': 131,\n",
       "    'missed': 51,\n",
       "    'spurious': 135,\n",
       "    'possible': 533,\n",
       "    'actual': 617,\n",
       "    'precision': 0.6750405186385737,\n",
       "    'recall': 0.7814258911819888,\n",
       "    'f1': 0.7243478260869565},\n",
       "   'strict': {'correct': 320,\n",
       "    'incorrect': 162,\n",
       "    'partial': 0,\n",
       "    'missed': 51,\n",
       "    'spurious': 135,\n",
       "    'possible': 533,\n",
       "    'actual': 617,\n",
       "    'precision': 0.5186385737439222,\n",
       "    'recall': 0.600375234521576,\n",
       "    'f1': 0.5565217391304347},\n",
       "   'exact': {'correct': 351,\n",
       "    'incorrect': 131,\n",
       "    'partial': 0,\n",
       "    'missed': 51,\n",
       "    'spurious': 135,\n",
       "    'possible': 533,\n",
       "    'actual': 617,\n",
       "    'precision': 0.5688816855753647,\n",
       "    'recall': 0.6585365853658537,\n",
       "    'f1': 0.6104347826086957}},\n",
       "  'Cuisine': {'ent_type': {'correct': 434,\n",
       "    'incorrect': 78,\n",
       "    'partial': 0,\n",
       "    'missed': 20,\n",
       "    'spurious': 64,\n",
       "    'possible': 532,\n",
       "    'actual': 576,\n",
       "    'precision': 0.7534722222222222,\n",
       "    'recall': 0.8157894736842105,\n",
       "    'f1': 0.7833935018050541},\n",
       "   'partial': {'correct': 442,\n",
       "    'incorrect': 0,\n",
       "    'partial': 70,\n",
       "    'missed': 20,\n",
       "    'spurious': 64,\n",
       "    'possible': 532,\n",
       "    'actual': 576,\n",
       "    'precision': 0.828125,\n",
       "    'recall': 0.8966165413533834,\n",
       "    'f1': 0.8610108303249098},\n",
       "   'strict': {'correct': 391,\n",
       "    'incorrect': 121,\n",
       "    'partial': 0,\n",
       "    'missed': 20,\n",
       "    'spurious': 64,\n",
       "    'possible': 532,\n",
       "    'actual': 576,\n",
       "    'precision': 0.6788194444444444,\n",
       "    'recall': 0.7349624060150376,\n",
       "    'f1': 0.7057761732851985},\n",
       "   'exact': {'correct': 442,\n",
       "    'incorrect': 70,\n",
       "    'partial': 0,\n",
       "    'missed': 20,\n",
       "    'spurious': 64,\n",
       "    'possible': 532,\n",
       "    'actual': 576,\n",
       "    'precision': 0.7673611111111112,\n",
       "    'recall': 0.8308270676691729,\n",
       "    'f1': 0.7978339350180504}}})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelMITRestaurant.evaluate_model(\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger():\n",
    "    def __init__(self, train, train_id, dev, dev_id, test, test_id, data_dict, labels_dict, loss, optimizer, metrics):\n",
    "        self.model = Sequential()\n",
    "        self.train = train\n",
    "        self.train_id = train_id\n",
    "        self.dev = dev\n",
    "        self.dev_id = dev_id\n",
    "        self.test = test\n",
    "        self.test_id = test_id\n",
    "        self.data_dict = data_dict\n",
    "        self.labels_dict = labels_dict\n",
    "        len_train = 0\n",
    "        for i in train:\n",
    "            len_train += len(i)\n",
    "        self.vocab_size = len_train + 1\n",
    "        self.classes = labels_dict.keys()\n",
    "        self.num_classes = len(labels_dict)\n",
    "        self.loss=loss\n",
    "        self.optimizer=optimizer\n",
    "        self.metrics=metrics\n",
    "        self.train_windows = []\n",
    "        self.train_windows_id = []\n",
    "        self.dev_windows = []\n",
    "        self.dev_windows_id = []\n",
    "        self.test_windows = []\n",
    "        self.test_windows_id = []\n",
    "        self.maxlen = 0\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def build_model(self, bidirectional=False):\n",
    "        self.model.add(Embedding(input_dim=self.vocab_size, output_dim=20, mask_zero=True, input_length=self.maxlen))\n",
    "        if bidirectional:\n",
    "            self.model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "        else:\n",
    "            self.model.add(LSTM(64, return_sequences=True))\n",
    "        self.model.add(TimeDistributed(Dense(units=self.num_classes, activation='softmax'))) \n",
    "        \n",
    "    def _padder(self, data, data_id):\n",
    "        # Obtenemos la longitud de la oración más larga\n",
    "        for i in range(len(data)):\n",
    "            if len(data[i]) > self.maxlen:\n",
    "                self.maxlen = len(data[i])\n",
    "\n",
    "        padded_data = pad_sequences(data, maxlen=self.maxlen, padding='post')\n",
    "        padded_data_id = pad_sequences(data_id, maxlen=self.maxlen, padding='post')\n",
    "        one_hot_data_id = to_categorical(padded_data_id, num_classes=self.num_classes)\n",
    "\n",
    "        return padded_data, one_hot_data_id\n",
    "          \n",
    "\n",
    "    def preprocessing(self):\n",
    "        self.train_windows, self.train_windows_id = self._padder(self.train, self.train_id)\n",
    "        self.dev_windows, self.dev_windows_id = self._padder(self.dev, self.dev_id)\n",
    "        self.test_windows, self.test_windows_id = self._padder(self.test, self.test_id)\n",
    "\n",
    "    def train_model(self):\n",
    "        train_tensor = tf.data.Dataset.from_tensor_slices((self.train_windows, self.train_windows_id))\n",
    "        train_tensor = train_tensor.batch(self.batch_size)\n",
    "        dev_tensor = tf.data.Dataset.from_tensor_slices((self.dev_windows, self.dev_windows_id))\n",
    "        dev_tensor = dev_tensor.batch(self.batch_size)\n",
    "\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "        self.model.fit(train_tensor, epochs=1, validation_data=dev_tensor)\n",
    "\n",
    "    def evaluate_model(self, task):\n",
    "        test_labels = [] # lista que almacena las etiquetas de los elementos de self.test_id\n",
    "        # Cambiamos las etiquetas de numérico a string\n",
    "        for i in self.test_id:\n",
    "            temp_phrase = []\n",
    "            for j in i:\n",
    "                for k, v in self.labels_dict.items():\n",
    "                    if j == v:\n",
    "                        temp_phrase.append(k)\n",
    "            test_labels.append(temp_phrase)\n",
    "        \n",
    "        lengths = []\n",
    "        for t in self.test_id:\n",
    "            lengths.append(len(t))\n",
    "\n",
    "        one_hot_dict = dict()\n",
    "        cnt_test_id = 0\n",
    "        for w in self.test_windows_id:\n",
    "            for i in range(len(w)):\n",
    "                if np.argmax(w[i]) not in one_hot_dict:\n",
    "                    one_hot_dict[np.argmax(w[i])] = self.test_id[cnt_test_id] # clave es el índice del elemento con valor 1 en one_hot_test_id[i], valor es el elemento de self.test_id -> al reves que en FFTagger\n",
    "                    cnt_test_id += 1\n",
    "\n",
    "        test_tensor = tf.data.Dataset.from_tensor_slices((self.test_windows, self.test_windows_id))\n",
    "        test_tensor = test_tensor.batch(self.batch_size)\n",
    "        \n",
    "        if task == \"PoS\":\n",
    "            loss, accuracy = self.model.evaluate(test_tensor, verbose=1)\n",
    "            return loss, accuracy\n",
    "        elif task == \"NER\":\n",
    "            loss, accuracy = self.model.evaluate(test_tensor, verbose=1)\n",
    "            pred = self.model.predict(test_tensor).astype(np.float32)\n",
    "\n",
    "            pred_wo_padding = []\n",
    "            for p in range(len(pred)):\n",
    "                pred_wo_padding.append(pred[p][:lengths[p]])\n",
    "            \n",
    "            # convertimos las predicciones a etiquetas numéricas\n",
    "            predictions = []\n",
    "            for phrase in pred_wo_padding:\n",
    "                temp_phrase = []\n",
    "                for i in range(len(phrase)):\n",
    "                    pos = np.argmax(phrase[i])\n",
    "                    for k, v in one_hot_dict.items():\n",
    "                        if pos == k:\n",
    "                            temp_phrase.append(v)\n",
    "                predictions.append(temp_phrase)\n",
    "\n",
    "            # convertimos las etiquetas numéricas a etiquetas de texto\n",
    "            pred_labels = []\n",
    "            for l in predictions:\n",
    "                tmp_phrase = []\n",
    "                for w in l:\n",
    "                    for k, v in self.labels_dict.items():\n",
    "                        if w == v:\n",
    "                            tmp_phrase.append(k)\n",
    "                pred_labels.append(tmp_phrase)\n",
    "            \n",
    "            # obtenemos las etiquetas únicas que hay en el conjunto de train y dev\n",
    "            numeric_tags = set()\n",
    "            for i in self.train_id:\n",
    "                numeric_tags = set(numeric_tags|set(i))\n",
    "            for j in self.dev_id:\n",
    "                numeric_tags = set(numeric_tags|set(j))\n",
    "            numeric_tags = list(numeric_tags)\n",
    "\n",
    "            # convertimos las etiquetas numéricas a etiquetas de texto\n",
    "            tags = []\n",
    "            for t in numeric_tags:\n",
    "                for k, v in self.labels_dict.items():\n",
    "                    if t == v:\n",
    "                        tags.append(k)\n",
    "\n",
    "            # Nos quedamos con las etiquetas de cada entidad\n",
    "            evaluator_tags = []\n",
    "            for tag in tags:\n",
    "                if tag == \"O\":\n",
    "                    continue\n",
    "                elif tag == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    evaluator_tag = tag[2:]\n",
    "\n",
    "                if evaluator_tag not in evaluator_tags:\n",
    "                    evaluator_tags.append(evaluator_tag)\n",
    "\n",
    "            evaluator = Evaluator(test_labels, pred=pred_labels, tags=evaluator_tags, loader=\"list\")\n",
    "            results, results_by_tag = evaluator.evaluate()\n",
    "            \n",
    "            return loss, accuracy, results, results_by_tag\n",
    "        else:\n",
    "            return \"Task not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 22s 552ms/step - loss: 2.7040 - accuracy: 0.2045 - val_loss: 2.5009 - val_accuracy: 0.2087\n"
     ]
    }
   ],
   "source": [
    "BDLSTMPartut = LSTMTagger(train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, test_PartTUT, test_id_PartTUT, PartTUT_data, PartTUT_labels, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "BDLSTMPartut.preprocessing()\n",
    "BDLSTMPartut.build_model(bidirectional=True)\n",
    "BDLSTMPartut.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDLSTMPartut.evaluate_model(\"PoS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMPartut = LSTMTagger(train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, test_PartTUT, test_id_PartTUT, PartTUT_data, PartTUT_labels, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "LSTMPartut.preprocessing()\n",
    "LSTMPartut.build_model(bidirectional=False)\n",
    "LSTMPartut.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMPartut.evaluate_model(\"PoS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDLSTMMITMovie = LSTMTagger(train_MITMovie, train_id_MITMovie, dev_MITMovie, dev_id_MITMovie, test_MITMovie, test_id_MITMovie, MITMovie_data, MITMovie_labels, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "BDLSTMMITMovie.preprocessing()\n",
    "BDLSTMMITMovie.build_model(bidirectional=True)\n",
    "BDLSTMMITMovie.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDLSTMMITMovie.evaluate_model(\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMMITMovie = LSTMTagger(train_MITMovie, train_id_MITMovie, dev_MITMovie, dev_id_MITMovie, test_MITMovie, test_id_MITMovie, MITMovie_data, MITMovie_labels, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "LSTMMITMovie.preprocessing()\n",
    "LSTMMITMovie.build_model(bidirectional=False)\n",
    "LSTMMITMovie.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMMITMovie.evaluate_model(\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 15s 74ms/step - loss: 0.5028 - accuracy: 0.6148 - val_loss: 0.3977 - val_accuracy: 0.6252\n"
     ]
    }
   ],
   "source": [
    "BDLSTMMITRestaurant = LSTMTagger(train_MITRestaurant, train_id_MITRestaurant, dev_MITRestaurant, dev_id_MITRestaurant, test_MITRestaurant, test_id_MITRestaurant, MITRestaurant_data, MITRestaurant_labels, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "BDLSTMMITRestaurant.preprocessing()\n",
    "BDLSTMMITRestaurant.build_model(bidirectional=True)\n",
    "BDLSTMMITRestaurant.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDLSTMMITRestaurant.evaluate_model(\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 14s 86ms/step - loss: 2.2546 - accuracy: 0.6136 - val_loss: 1.7111 - val_accuracy: 0.6215\n"
     ]
    }
   ],
   "source": [
    "LSTMMITRestaurant = LSTMTagger(train_MITRestaurant, train_id_MITRestaurant, dev_MITRestaurant, dev_id_MITRestaurant, test_MITRestaurant, test_id_MITRestaurant, MITRestaurant_data, MITRestaurant_labels, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "LSTMMITRestaurant.preprocessing()\n",
    "LSTMMITRestaurant.build_model(bidirectional=False)\n",
    "LSTMMITRestaurant.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMMITRestaurant.evaluate_model(\"NER\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5f83275ee69d87b8221315355688642ba9d798c52e57f09be426c31a39c1344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
