{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class txtReader:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def read_split(self):\n",
    "        with open(self.filename, 'r') as f:\n",
    "            file_read = f.read()\n",
    "        efg = []\n",
    "        lines = file_read.split('\\n')\n",
    "        efg.append(lines)\n",
    "        text = []\n",
    "        text_id = []\n",
    "        for i in lines:\n",
    "            if i != '':\n",
    "                word = i.split('\\t')\n",
    "                text.append(word[0])\n",
    "                text_id.append(word[1])\n",
    "\n",
    "        return text, text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alphabet:\n",
    "    def __init__(self, train_file, dev_file, test_file):\n",
    "        self.train_file = train_file\n",
    "        self.dev_file = dev_file\n",
    "        self.test_file = test_file\n",
    "        self.data = dict()\n",
    "        self.labels = dict()\n",
    "\n",
    "    def read_split(self):\n",
    "        text_files = []\n",
    "        for text_file in [self.train_file, self.dev_file, self.test_file]:\n",
    "            txt = txtReader(text_file)\n",
    "            text, text_id = txt.read_split()\n",
    "            text_files.append(text)\n",
    "            text_files.append(text_id)\n",
    "\n",
    "        return text_files[0], text_files[1], text_files[2], text_files[3], text_files[4], text_files[5]\n",
    "    \n",
    "    def _tagger(self, dataset, cnt, dictionary):\n",
    "        for i in dataset:\n",
    "            # obtener indice de i en dataset\n",
    "            pos = dataset.index(i)\n",
    "            if i not in dictionary:\n",
    "                dictionary[i] = cnt\n",
    "                dataset[pos] = cnt\n",
    "                cnt += 1\n",
    "            else:\n",
    "                dataset[pos] = dictionary[i]\n",
    "\n",
    "        return dataset, cnt, dictionary \n",
    "\n",
    "    def labelEncoder(self):\n",
    "        train, train_id, dev, dev_id, test, test_id = self.read_split()\n",
    "        cnt = 1\n",
    "        cnt_id = 1\n",
    "\n",
    "        train, cnt, self.data = self._tagger(train, cnt, self.data) \n",
    "        train_id, cnt_id, self.labels = self._tagger(train_id, cnt_id, self.labels)\n",
    "        dev, cnt, self.data = self._tagger(dev, cnt, self.data)\n",
    "        dev_id, cnt_id, self.labels = self._tagger(dev_id, cnt_id, self.labels)\n",
    "        \n",
    "        for te in test:\n",
    "            pos = test.index(te)\n",
    "            if te not in self.data:\n",
    "                self.data[te] = -1 # -1 indica que la palabra es desconocida\n",
    "                test[pos] = -1\n",
    "            else:\n",
    "                test[pos] = self.data[te]\n",
    "\n",
    "        for te_id in test_id:\n",
    "            pos_id = test_id.index(te_id)\n",
    "            if te_id not in self.labels:  \n",
    "                self.labels[te_id] = -1\n",
    "                test_id[pos_id] = self.labels[te_id]\n",
    "            else:\n",
    "                test_id[pos_id] = self.labels[te_id]  \n",
    "\n",
    "        return train, train_id, dev, dev_id, test, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_id, dev, dev_id, test, test_id = alphabet('materiales_practica/datasets/PartTUT/train.txt', 'materiales_practica/datasets/PartTUT/dev.txt', 'materiales_practica/datasets/PartTUT/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTagger():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def model(self, n):\n",
    "        model = tf.keras.model.Sequential()\n",
    "        model.add(tf.keras.Input(shape=(n*2+1,)))\n",
    "        model.add(tf.keras.layers.Embedding(input_dim=1, output_dim=20, input_length=n*2+1))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(1, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "    def train(self, train, train_id, dev, dev_id, n, model):\n",
    "        train = np.concatenate((np.zeros(n), train, np.zeros(n)))\n",
    "        dev = np.concatenate((np.zeros(n), dev, np.zeros(n)))\n",
    "\n",
    "        # almacenar ventanas de tamaño n*2+1 en una lista de listas para cada conjunto\n",
    "        train_windows = []\n",
    "        dev_windows = []\n",
    "\n",
    "        for i in range(n, len(train) - n):\n",
    "            # Quedarme con todos los elementos de text que estén entre i-n y i+n\n",
    "            data = train[i-n:i+n+1]\n",
    "            train_windows.append(data)\n",
    "        \n",
    "        for i in range(n, len(dev) - n):\n",
    "            data = dev[i-n:i+n+1]\n",
    "            dev_windows.append(data)\n",
    "\n",
    "        model.fit(train_windows, train_id, epochs=10, validation_data=(dev_windows, dev_id), verbose=1)\n",
    "\n",
    "        return model\n",
    "\n",
    "    # def predict(self, model, test, test_id, n):\n",
    "    #     test = np.concatenate((np.zeros(n), test, np.zeros(n)))\n",
    "    #     test_id = np.concatenate((np.zeros(n), test_id, np.zeros(n)))\n",
    "    #     for i in range(n, len(text) - n):\n",
    "    #         pred = model.predict(test)\n",
    "\n",
    "\n",
    "    #     return pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5f83275ee69d87b8221315355688642ba9d798c52e57f09be426c31a39c1344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
