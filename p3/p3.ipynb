{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, LSTM, TimeDistributed\n",
    "from nervaluate import Evaluator\n",
    "from keras.utils import to_categorical\n",
    "import nltk \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class txtReader:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def read_split(self):\n",
    "        with open(self.filename, 'r') as f:\n",
    "            file_read = f.read()\n",
    "        efg = []\n",
    "        lines = file_read.split('\\n')\n",
    "        efg.append(lines)\n",
    "        text = []\n",
    "        text_id = []\n",
    "        for i in lines:\n",
    "            i = i.strip()\n",
    "            if i != '':\n",
    "                word = i.split('\\t')\n",
    "                text.append(word[0])\n",
    "                text_id.append(word[1])\n",
    "\n",
    "        return text, text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alphabet:\n",
    "    def __init__(self, train_file, dev_file, test_file):\n",
    "        self.train_file = train_file\n",
    "        self.dev_file = dev_file\n",
    "        self.test_file = test_file\n",
    "        self.data = dict()\n",
    "        self.labels = dict()\n",
    "\n",
    "    def read_split(self):\n",
    "        text_files = []\n",
    "        for text_file in [self.train_file, self.dev_file, self.test_file]:\n",
    "            txt = txtReader(text_file)\n",
    "            text, text_id = txt.read_split()\n",
    "            text_files.append(text)\n",
    "            text_files.append(text_id)\n",
    "\n",
    "        return text_files[0], text_files[1], text_files[2], text_files[3], text_files[4], text_files[5]\n",
    "    \n",
    "    def _tagger(self, dataset, cnt, dictionary):\n",
    "        for i in dataset:\n",
    "            pos = dataset.index(i)\n",
    "            if i not in dictionary:\n",
    "                dictionary[i] = cnt\n",
    "                dataset[pos] = cnt\n",
    "                cnt += 1\n",
    "            else:\n",
    "                dataset[pos] = dictionary[i]\n",
    "\n",
    "        return dataset, cnt, dictionary \n",
    "\n",
    "    def labelEncoder(self):\n",
    "        train, train_id, dev, dev_id, test, test_id = self.read_split()\n",
    "        cnt = 1\n",
    "        cnt_id = 0\n",
    "\n",
    "        train, cnt, self.data = self._tagger(train, cnt, self.data) \n",
    "        train_id, cnt_id, self.labels = self._tagger(train_id, cnt_id, self.labels)\n",
    "        dev, cnt, self.data = self._tagger(dev, cnt, self.data)\n",
    "        dev_id, cnt_id, self.labels = self._tagger(dev_id, cnt_id, self.labels)\n",
    "        \n",
    "        for te in test:\n",
    "            pos = test.index(te)\n",
    "            if te not in self.data:\n",
    "                self.data[te] = len(train) # indica que la palabra es desconocida\n",
    "                test[pos] = self.data[te]\n",
    "            else:\n",
    "                test[pos] = self.data[te]\n",
    "\n",
    "        for te_id in test_id:\n",
    "            pos_id = test_id.index(te_id)\n",
    "            if te_id not in self.labels:  \n",
    "                self.labels[te_id] = len(train_id) \n",
    "                test_id[pos_id] = self.labels[te_id]\n",
    "            else:\n",
    "                test_id[pos_id] = self.labels[te_id]  \n",
    "\n",
    "        return train, train_id, dev, dev_id, test, test_id, self.data, self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, test_PartTUT, test_id_PartTUT, PartTUT_data, PartTUT_labels = alphabet('materiales_practica/datasets/PartTUT/train.txt', 'materiales_practica/datasets/PartTUT/dev.txt', 'materiales_practica/datasets/PartTUT/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MITMovie, train_id_MITMovie, dev_MITMovie, dev_id_MITMovie, test_MITMovie, test_id_MITMovie, MITMovie_data, MITMovie_labels = alphabet('materiales_practica/datasets/MITMovie/train.txt', 'materiales_practica/datasets/MITMovie/dev.txt', 'materiales_practica/datasets/MITMovie/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MITRestaurant, train_id_MITRestaurant, dev_MITRestaurant, dev_id_MITRestaurant, test_MITRestaurant, test_id_MITRestaurant, MITRestaurant_data, MITRestaurant_labels = alphabet('materiales_practica/datasets/MITRestaurant/train.txt', 'materiales_practica/datasets/MITRestaurant/dev.txt', 'materiales_practica/datasets/MITRestaurant/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTagger():\n",
    "    def __init__(self, train, train_id, dev, dev_id, test, test_id, labels_dict, n, loss, optimizer, metrics): # weighted_metrics\n",
    "        self.model = Sequential()\n",
    "        self.train = train\n",
    "        self.train_id = train_id\n",
    "        self.dev = dev\n",
    "        self.dev_id = dev_id\n",
    "        self.test = test\n",
    "        self.test_id = test_id\n",
    "        self.test_labels_dict = labels_dict\n",
    "        self.n = n\n",
    "        self.vocab_size = len(self.train) + 1 # +1 por valores desconocidos en test\n",
    "        self.classes = set(set(self.train_id) | set(self.dev_id))\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        # self.weighted_metrics = weighted_metrics\n",
    "        self.train_windows = []\n",
    "        self.dev_windows = []\n",
    "        self.test_windows = []\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def build_model(self): \n",
    "        self.model.add(Input(shape=(self.n*2+1,), dtype=tf.int32))\n",
    "        self.model.add(Embedding(input_dim = self.vocab_size, output_dim=20, mask_zero=True, input_length=self.n*2+1))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "    def train_model(self, task):\n",
    "        padding = []\n",
    "        for i in range(self.n):\n",
    "            padding.append(0)\n",
    "        self.train = padding + self.train + padding\n",
    "        self.dev = padding + self.dev + padding\n",
    "\n",
    "        # almacenar ventanas de tamaño n*2+1 en una lista de listas para cada conjunto\n",
    "        for i in range(self.n, len(self.train) - self.n):\n",
    "            data = self.train[i-self.n:i+self.n+1]\n",
    "            self.train_windows.append(data)\n",
    "        \n",
    "        for i in range(self.n, len(self.dev) - self.n):\n",
    "            data = self.dev[i-self.n:i+self.n+1]\n",
    "            self.dev_windows.append(data) \n",
    "\n",
    "        one_hot_train_id = to_categorical(self.train_id, num_classes=self.num_classes)\n",
    "        one_hot_dev_id = to_categorical(self.dev_id, num_classes=self.num_classes)\n",
    "        \n",
    "        train_tensor = tf.data.Dataset.from_tensor_slices((self.train_windows, one_hot_train_id))\n",
    "        train_tensor = train_tensor.batch(self.batch_size)\n",
    "        dev_tensor = tf.data.Dataset.from_tensor_slices((self.dev_windows, one_hot_dev_id))\n",
    "        dev_tensor = dev_tensor.batch(self.batch_size)\n",
    "\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "        self.model.fit(train_tensor, epochs=1, validation_data=dev_tensor, verbose=1)\n",
    "\n",
    "    def evaluate_model(self, task):\n",
    "        padding = []\n",
    "        for i in range(self.n):\n",
    "            padding.append(0)\n",
    "        self.test = padding + self.test + padding\n",
    "\n",
    "        self.test_windows = []\n",
    "        for i in range(self.n, len(self.test) - self.n):\n",
    "            data = self.test[i-self.n:i+self.n+1]\n",
    "            self.test_windows.append(data)\n",
    "\n",
    "        test_labels = []\n",
    "        \n",
    "        for i in self.test_id:\n",
    "            for k, v in self.test_labels_dict.items():\n",
    "                if i == v:\n",
    "                    test_labels.append(k)\n",
    "\n",
    "        one_hot_test_id = to_categorical(self.test_id, num_classes=self.num_classes)\n",
    "        \n",
    "        one_hot_dict = dict()\n",
    "        # obtener diccionario que almacene pares k, v para cada one hot y cada elemento de self.test_id\n",
    "        for i in range(len(self.test_id)):\n",
    "            if self.test_id[i] not in one_hot_dict:\n",
    "                one_hot_dict[self.test_id[i]] = np.argmax(one_hot_test_id[i]) # devuelve el índice del elemento con valor 1 en one_hot_test_id[i]\n",
    "\n",
    "        test_tensor = tf.data.Dataset.from_tensor_slices((self.test_windows, one_hot_test_id))\n",
    "        test_tensor = test_tensor.batch(self.batch_size)\n",
    "        \n",
    "        if task == \"PoS\":\n",
    "            loss, accuracy = self.model.evaluate(test_tensor, verbose=1)\n",
    "            return loss, accuracy\n",
    "        elif task == \"NER\":\n",
    "            loss, accuracy = self.model.evaluate(test_tensor, verbose=1)\n",
    "            pred = self.model.predict(test_tensor).astype(np.float32)\n",
    "            # para cada elemento de pred, obtener codificación one hot  \n",
    "\n",
    "            predictions = []\n",
    "            for p in pred:\n",
    "                pos = np.argmax(p)\n",
    "                for k, v in one_hot_dict.items():\n",
    "                    if pos == v:\n",
    "                        predictions.append(k)\n",
    "\n",
    "            numeric_tags = list(set(set(self.train_id) | set(self.dev_id)))\n",
    "\n",
    "            # convertimos las etiquetas numéricas a etiquetas de texto\n",
    "            tags = []\n",
    "            for t in numeric_tags:\n",
    "                for k, v in self.test_labels_dict.items():\n",
    "                    if t == v:\n",
    "                        tags.append(k)\n",
    "\n",
    "            evaluator_tags = []\n",
    "            for tag in tags:\n",
    "                if tag == \"O\":\n",
    "                    evaluator_tag = tag\n",
    "                else:\n",
    "                    evaluator_tag = tag[2:]\n",
    "\n",
    "                if evaluator_tag not in evaluator_tags:\n",
    "                    evaluator_tags.append(evaluator_tag)\n",
    "\n",
    "            pred_labels = []\n",
    "            for l in predictions:\n",
    "                for k, v in self.test_labels_dict.items():\n",
    "                    if l == v:\n",
    "                        pred_labels.append(k)\n",
    "\n",
    "            evaluator = Evaluator(test_labels, pred=pred_labels, tags=evaluator_tags, loader=\"list\")\n",
    "            results, results_by_tag = evaluator.evaluate()\n",
    "            \n",
    "            return loss, accuracy, results, results_by_tag\n",
    "        else:\n",
    "            return \"Task not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4351/4351 [==============================] - 44s 10ms/step - loss: 0.6411 - accuracy: 0.8154 - val_loss: 0.4009 - val_accuracy: 0.8733\n"
     ]
    }
   ],
   "source": [
    "modelPartTUT = FFTagger(train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, test_PartTUT, test_id_PartTUT,  PartTUT_labels, 2, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "modelPartTUT.build_model()\n",
    "modelPartTUT.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 0s 941us/step - loss: 0.3467 - accuracy: 0.8964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3467119336128235, 0.8964201807975769)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelPartTUT.evaluate_model(\"PoS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14283/14283 [==============================] - 914s 64ms/step - loss: 0.7141 - accuracy: 0.7890 - val_loss: 0.5518 - val_accuracy: 0.8295\n"
     ]
    }
   ],
   "source": [
    "modelMITMovie = FFTagger(train_MITMovie, train_id_MITMovie, dev_MITMovie, dev_id_MITMovie, test_MITMovie, test_id_MITMovie, MITMovie_labels, 2, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "modelMITMovie.build_model()\n",
    "modelMITMovie.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelMITMovie.evaluate_model(\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 29s 29ms/step - loss: 1.0134 - accuracy: 0.7328 - val_loss: 0.5847 - val_accuracy: 0.8391\n"
     ]
    }
   ],
   "source": [
    "modelMITRestaurant = FFTagger(train_MITRestaurant, train_id_MITRestaurant, dev_MITRestaurant, dev_id_MITRestaurant, test_MITRestaurant, test_id_MITRestaurant, MITRestaurant_labels, 2, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "modelMITRestaurant.build_model()\n",
    "modelMITRestaurant.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/223 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.8346\n",
      "223/223 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6089786291122437,\n",
       " 0.8345959782600403,\n",
       " {'ent_type': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 0,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0},\n",
       "  'partial': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 0,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0},\n",
       "  'strict': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 0,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0},\n",
       "  'exact': {'correct': 0,\n",
       "   'incorrect': 0,\n",
       "   'partial': 0,\n",
       "   'missed': 0,\n",
       "   'spurious': 0,\n",
       "   'possible': 0,\n",
       "   'actual': 0,\n",
       "   'precision': 0,\n",
       "   'recall': 0,\n",
       "   'f1': 0}},\n",
       " {'O': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}},\n",
       "  'Rating': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}},\n",
       "  'Location': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}},\n",
       "  'Restaurant_Name': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}},\n",
       "  'Price': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}},\n",
       "  'Hours': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}},\n",
       "  'Dish': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}},\n",
       "  'Amenity': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}},\n",
       "  'Cuisine': {'ent_type': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'partial': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'strict': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0},\n",
       "   'exact': {'correct': 0,\n",
       "    'incorrect': 0,\n",
       "    'partial': 0,\n",
       "    'missed': 0,\n",
       "    'spurious': 0,\n",
       "    'possible': 0,\n",
       "    'actual': 0,\n",
       "    'precision': 0,\n",
       "    'recall': 0,\n",
       "    'f1': 0}}})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelMITRestaurant.evaluate_model(\"NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger():\n",
    "    def __init__(self, train, train_id, dev, dev_id, test, test_id, data_dict, loss, optimizer, metrics):\n",
    "        self.model = Sequential()\n",
    "        self.train = train\n",
    "        self.train_id = train_id\n",
    "        self.dev = dev\n",
    "        self.dev_id = dev_id\n",
    "        self.test = test\n",
    "        self.test_id = test_id\n",
    "        self.data_dict = data_dict\n",
    "        self.vocab_size = len(set(self.train)) + 1\n",
    "        self.classes = set(set(self.train_id) | set(self.dev_id))\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.loss=loss\n",
    "        self.optimizer=optimizer\n",
    "        self.metrics=metrics\n",
    "        self.train_windows = []\n",
    "        self.train_windows_id = []\n",
    "        self.dev_windows = []\n",
    "        self.dev_windows_id = []\n",
    "        self.test_windows = []\n",
    "        self.test_windows_id = []\n",
    "        self.point_id = self.data_dict[\".\"]\n",
    "        self.question_mark_id = self.data_dict[\"?\"]\n",
    "        self.exclamation_mark_id = self.data_dict[\"!\"]\n",
    "        self.maxlen = 0\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def build_model(self, bidirectional=False):\n",
    "        self.model.add(Embedding(input_dim=self.vocab_size, output_dim=20, mask_zero=True, input_length=self.maxlen))\n",
    "        if bidirectional:\n",
    "            self.model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "        else:\n",
    "            self.model.add(LSTM(64, return_sequences=True))\n",
    "        self.model.add(TimeDistributed(Dense(units=self.num_classes, activation='softmax')))\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def _window_maker(self, data, data_id, windows, windows_id):\n",
    "        maxlen = 0\n",
    "        window = []\n",
    "        window_id = []\n",
    "        # Separamos en oraciones\n",
    "        for i in range(len(data)):\n",
    "            window.append(data[i])\n",
    "            window_id.append(data_id[i])\n",
    "            if data[i] == self.point_id or data[i] == self.question_mark_id or data[i] == self.exclamation_mark_id:\n",
    "                windows.append(window)\n",
    "                windows_id.append(window_id)\n",
    "                if len(window) > maxlen:\n",
    "                    maxlen = len(window)\n",
    "                window = []\n",
    "                window_id = []\n",
    "        # Añadimos padding\n",
    "        padded_windows = pad_sequences(windows, maxlen=maxlen, padding='post')\n",
    "        padded_windows_id = pad_sequences(windows_id, maxlen=maxlen, padding='post')\n",
    "        # Convertimos las etiquetas a one-hot\n",
    "        one_hot_windows_id = to_categorical(padded_windows_id, num_classes=self.num_classes)\n",
    "        return padded_windows, one_hot_windows_id, maxlen      \n",
    "\n",
    "    def preprocessing(self):\n",
    "        self.train_windows, self.train_windows_id, self.maxlen = self._window_maker(self.train, self.train_id, self.train_windows, self.train_windows_id)\n",
    "        self.dev_windows, self.dev_windows_id, _ = self._window_maker(self.dev, self.dev_id, self.dev_windows, self.dev_windows_id)\n",
    "        self.test_windows, self.test_windows_id, _ = self._window_maker(self.test, self.test_id, self.test_windows, self.test_windows_id)\n",
    "\n",
    "    def train_model(self):\n",
    "        # print('tw',len(self.train_windows))\n",
    "        # print('t',len(self.train))\n",
    "        # train_tensor = tf.data.Dataset.from_tensor_slices((self.train_windows, self.train_windows_id))\n",
    "        # train_tensor = train_tensor.batch(self.batch_size)\n",
    "        # dev_tensor = tf.data.Dataset.from_tensor_slices((self.dev_windows, self.dev_windows_id))\n",
    "        # dev_tensor = dev_tensor.batch(self.batch_size)\n",
    "\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=self.metrics)\n",
    "        # self.model.fit(train_tensor, epochs=1, validation_data=dev_tensor)\n",
    "        self.model.fit(self.train_windows, self.train_windows_id, epochs=1, validation_data=(self.dev_windows, self.dev_windows_id))\n",
    "\n",
    "\n",
    "\n",
    "    # def evaluate_model(self):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 314, 20)           139120    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 314, 64)           21760     \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 314, 17)          1105      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,985\n",
      "Trainable params: 161,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "53/53 [==============================] - ETA: 0s - loss: 2.6032 - accuracy: 0.2075"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_23\" is incompatible with the layer: expected shape=(None, 314), found shape=(None, 50)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Desktop\\UDC\\PLE\\PLE\\p3\\p3.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/UDC/PLE/PLE/p3/p3.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m LSTMPartut\u001b[39m.\u001b[39mpreprocessing()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/UDC/PLE/PLE/p3/p3.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m LSTMPartut\u001b[39m.\u001b[39mbuild_model(bidirectional\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/UDC/PLE/PLE/p3/p3.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m LSTMPartut\u001b[39m.\u001b[39;49mtrain_model()\n",
      "\u001b[1;32mc:\\Users\\Usuario\\Desktop\\UDC\\PLE\\PLE\\p3\\p3.ipynb Cell 15\u001b[0m in \u001b[0;36mLSTMTagger.train_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/UDC/PLE/PLE/p3/p3.ipynb#X20sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss, optimizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, metrics\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/UDC/PLE/PLE/p3/p3.ipynb#X20sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# self.model.fit(train_tensor, epochs=1, validation_data=dev_tensor)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/UDC/PLE/PLE/p3/p3.ipynb#X20sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_windows, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_windows_id, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdev_windows, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdev_windows_id))\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemyqn439n.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\training.py\", line 1788, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Usuario\\.conda\\envs\\ple\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_23\" is incompatible with the layer: expected shape=(None, 314), found shape=(None, 50)\n"
     ]
    }
   ],
   "source": [
    "LSTMPartut = LSTMTagger(train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, test_PartTUT, test_id_PartTUT, PartTUT_data, 'categorical_crossentropy', 'adam', ['accuracy'])\n",
    "LSTMPartut.preprocessing()\n",
    "LSTMPartut.build_model(bidirectional=False)\n",
    "LSTMPartut.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5f83275ee69d87b8221315355688642ba9d798c52e57f09be426c31a39c1344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
