{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, LSTM\n",
    "from nervaluate import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class txtReader:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def read_split(self):\n",
    "        with open(self.filename, 'r') as f:\n",
    "            file_read = f.read()\n",
    "        efg = []\n",
    "        lines = file_read.split('\\n')\n",
    "        efg.append(lines)\n",
    "        text = []\n",
    "        text_id = []\n",
    "        for i in lines:\n",
    "            if i != '':\n",
    "                word = i.split('\\t')\n",
    "                text.append(word[0])\n",
    "                text_id.append(word[1])\n",
    "\n",
    "        return text, text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alphabet:\n",
    "    def __init__(self, train_file, dev_file, test_file):\n",
    "        self.train_file = train_file\n",
    "        self.dev_file = dev_file\n",
    "        self.test_file = test_file\n",
    "        self.data = dict()\n",
    "        self.labels = dict()\n",
    "\n",
    "    def read_split(self):\n",
    "        text_files = []\n",
    "        for text_file in [self.train_file, self.dev_file, self.test_file]:\n",
    "            txt = txtReader(text_file)\n",
    "            text, text_id = txt.read_split()\n",
    "            text_files.append(text)\n",
    "            text_files.append(text_id)\n",
    "\n",
    "        return text_files[0], text_files[1], text_files[2], text_files[3], text_files[4], text_files[5]\n",
    "    \n",
    "    def _tagger(self, dataset, cnt, dictionary):\n",
    "        for i in dataset:\n",
    "            # obtener indice de i en dataset\n",
    "            pos = dataset.index(i)\n",
    "            if i not in dictionary:\n",
    "                dictionary[i] = cnt\n",
    "                dataset[pos] = cnt\n",
    "                cnt += 1\n",
    "            else:\n",
    "                dataset[pos] = dictionary[i]\n",
    "\n",
    "        return dataset, cnt, dictionary \n",
    "\n",
    "    def labelEncoder(self):\n",
    "        train, train_id, dev, dev_id, test, test_id = self.read_split()\n",
    "        cnt = 1\n",
    "        cnt_id = 1\n",
    "\n",
    "        train, cnt, self.data = self._tagger(train, cnt, self.data) \n",
    "        train_id, cnt_id, self.labels = self._tagger(train_id, cnt_id, self.labels)\n",
    "        dev, cnt, self.data = self._tagger(dev, cnt, self.data)\n",
    "        dev_id, cnt_id, self.labels = self._tagger(dev_id, cnt_id, self.labels)\n",
    "        \n",
    "        for te in test:\n",
    "            pos = test.index(te)\n",
    "            if te not in self.data:\n",
    "                self.data[te] = -1 # -1 indica que la palabra es desconocida\n",
    "                test[pos] = -1\n",
    "            else:\n",
    "                test[pos] = self.data[te]\n",
    "\n",
    "        for te_id in test_id:\n",
    "            pos_id = test_id.index(te_id)\n",
    "            if te_id not in self.labels:  \n",
    "                self.labels[te_id] = -1\n",
    "                test_id[pos_id] = self.labels[te_id]\n",
    "            else:\n",
    "                test_id[pos_id] = self.labels[te_id]  \n",
    "\n",
    "        return train, train_id, dev, dev_id, test, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, test_PartTUT, test_id_PartTUT = alphabet('materiales_practica/datasets/PartTUT/train.txt', 'materiales_practica/datasets/PartTUT/dev.txt', 'materiales_practica/datasets/PartTUT/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MITMovie, train_id_MITMovie, dev_MITMovie, dev_id_MITMovie, test_MITMovie, test_id_MITMovie = alphabet('materiales_practica/datasets/MITMovie/train.txt', 'materiales_practica/datasets/MITMovie/dev.txt', 'materiales_practica/datasets/MITMovie/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MITRestaurant, train_id_MITRestaurant, dev_MITRestaurant, dev_id_MITRestaurant, test_MITRestaurant, test_id_MITRestaurant = alphabet('materiales_practica/datasets/MITRestaurant/train.txt', 'materiales_practica/datasets/MITRestaurant/dev.txt', 'materiales_practica/datasets/MITRestaurant/test.txt').labelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTagger():\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "    def build_model(self, n): \n",
    "        self.model.add(Input(shape=(n*2+1,)))\n",
    "        self.model.add(Embedding(input_dim=1, output_dim=20, input_length=n*2+1))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        self.model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    def train(self, train, train_id, dev, dev_id, n, loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], weighted_metrics=[]):\n",
    "        # train = np.concatenate((np.zeros(n), train, np.zeros(n)))\n",
    "        # dev = np.concatenate((np.zeros(n), dev, np.zeros(n)))\n",
    "        padding = []\n",
    "        for i in range(n):\n",
    "            padding.append(0)\n",
    "        train = padding + train + padding\n",
    "        dev = padding + dev + padding\n",
    "\n",
    "        # almacenar ventanas de tama√±o n*2+1 en una lista de listas para cada conjunto\n",
    "        train_windows = []\n",
    "        dev_windows = []\n",
    "\n",
    "        for i in range(n, len(train) - n):\n",
    "            data = train[i-n:i+n+1]\n",
    "            train_windows.append(data)\n",
    "        \n",
    "        for i in range(n, len(dev) - n):\n",
    "            data = dev[i-n:i+n+1]\n",
    "            dev_windows.append(data) \n",
    "\n",
    "        print(type(train_windows))\n",
    "        print(type(train_windows[0]))\n",
    "\n",
    "        print(\"train_windows: \", len(train_windows))\n",
    "        print(\"id\", len(train_id))\n",
    "        print(\"dev_windows: \", len(dev_windows))\n",
    "        print(\"id\", len(dev_id))\n",
    "\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics, weighted_metrics=weighted_metrics)\n",
    "        self.model.fit(train_windows, train_id, epochs=10, validation_data=(dev_windows, dev_id), verbose=0)\n",
    "\n",
    "    def evaluate(self, test, test_id, n, task):\n",
    "        test = np.concatenate((np.zeros(n), test, np.zeros(n)))\n",
    "        if task == \"PoS\":\n",
    "            return self.model.evaluate(test, test_id)\n",
    "        elif task == \"NER\":\n",
    "            evaluator = Evaluator(test_id, test, tags=['ent_type', 'partial', 'exact', 'strict']).evaluate()\n",
    "            \n",
    "            return self.model.evaluate(test, test_id), evaluator['ent_type']['f1'], evaluator['partial']['f1'], evaluator['exact']['f1'], evaluator['strict']['f1']\n",
    "        else:\n",
    "            return \"Task not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "train_windows:  43511\n",
      "id 43503\n",
      "dev_windows:  2730\n",
      "id 2722\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 43511\n  y sizes: 43503\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrador\\Desktop\\Q6\\PLE\\PLE\\p3\\p3.ipynb Celda 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrador/Desktop/Q6/PLE/PLE/p3/p3.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m modelPartTUT \u001b[39m=\u001b[39m FFTagger()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrador/Desktop/Q6/PLE/PLE/p3/p3.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m modelPartTUT\u001b[39m.\u001b[39mbuild_model(\u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrador/Desktop/Q6/PLE/PLE/p3/p3.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m modelPartTUT\u001b[39m.\u001b[39;49mtrain(train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, \u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Administrador\\Desktop\\Q6\\PLE\\PLE\\p3\\p3.ipynb Celda 8\u001b[0m in \u001b[0;36mFFTagger.train\u001b[1;34m(self, train, train_id, dev, dev_id, n, loss, optimizer, metrics, weighted_metrics)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrador/Desktop/Q6/PLE/PLE/p3/p3.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(dev_id))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Administrador/Desktop/Q6/PLE/PLE/p3/p3.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mloss, optimizer\u001b[39m=\u001b[39moptimizer, metrics\u001b[39m=\u001b[39mmetrics, weighted_metrics\u001b[39m=\u001b[39mweighted_metrics)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Administrador/Desktop/Q6/PLE/PLE/p3/p3.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(train_windows, train_id, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(dev_windows, dev_id), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 43511\n  y sizes: 43503\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "modelPartTUT = FFTagger()\n",
    "modelPartTUT.build_model(2)\n",
    "modelPartTUT.train(train_PartTUT, train_id_PartTUT, dev_PartTUT, dev_id_PartTUT, 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5f83275ee69d87b8221315355688642ba9d798c52e57f09be426c31a39c1344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
